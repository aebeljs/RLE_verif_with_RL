[main]
; number of steps for which the experiment is run
num_steps = 1000

; number of functional events tracked
num_events = 10

; weightage of each event for reward computation
reward_function = [0,0,0,0,2,0,0,0,0,0]

; set log_step as 0 for logging just aggregated results and 1 for logging details in each step
log_step = 0

; set mode as 0 to generate the random baseline without RL and 1 for using RL
mode = 1

; specify the stable_baselines3 algorithm to be used from SAC, DDPG and TD3
algorithm = SAC

; fsm_states contains the regex patterns for state-based binary sequence generation (refer to the docs for more details)
fsm_states = []

; provide the discrete parameter keys here
discrete_params = ['master_wr_addr_0', 'master_wr_addr_1']

; provide the bounds of the continuous dimensions of the action space in this section
; if multiple dimensions are there, provide the list of bounds for each dimension
; eg: lower_bounds = [0, 0, 1] and upper_bounds = [1, 1, 3]
[continuous]
lower_bounds = []
upper_bounds = []

; provide the list of valid discrete values for each parameter in this section
[discrete]
master_wr_addr_0 = [4096, 8192, 12288, 16384, 20480, 24576, 28672, 32768, 36864, 40960]
master_wr_addr_1 = [8191, 12287, 16383, 20479, 24575, 28671, 32767, 36863, 40959, 45055]

; refer to the stable_baselines3 SAC documentation for the hyperparameters in this section
[RL]
policy = 'MlpPolicy'
learning_starts = 300
learning_rate = 0.0003
train_freq = (1, 'episode')
verbose = 1
