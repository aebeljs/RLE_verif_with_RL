[main]
; number of steps for which the experiment is run
num_steps = 1000

; number of functional events tracked
num_events = 4

; weightage of each event for reward computation
reward_function = [0, 0, 0, 1]

; set log_step as 0 for logging just aggregated results and 1 for logging details in each step
log_step = 0

; set mode as 0 to generate the random baseline without RL and 1 for using RL
mode = 1

; specify the stable_baselines3 algorithm to be used from SAC, DDPG and TD3
algorithm = SAC

; fsm_states contains the regex patterns for state-based binary sequence generation (refer to the docs for more details)
fsm_states = ['.']

; provide the discrete parameter keys here
discrete_params = ['count_width', 'fmap_len']

; provide the bounds of the continuous dimensions of the action space in this section
; if multiple dimensions are there, provide the list of bounds for each dimension
; eg: lower_bounds = [0, 0, 1] and upper_bounds = [1, 1, 3]
[continuous]
lower_bounds = [0]
upper_bounds = [1]

; provide the list of valid discrete values for each parameter in this section
[discrete]
count_width = [1, 2, 3, 4, 5, 6, 7, 8]
fmap_len = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]

; refer to the stable_baselines3 documentation for the hyperparameters of the chosen algorithm in this section
[RL]
policy = 'MlpPolicy'
learning_starts = 100
learning_rate = 0.0003
train_freq = (1, 'episode')
verbose = 1
